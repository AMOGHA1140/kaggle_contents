{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2523e1b",
   "metadata": {},
   "source": [
    "# First attempt at MNIST classifier\n",
    "This code implements a simple neural network of 2 hidden layers of 16 units each with ReLU activation, and 1 output layer of 10 units with softmax activation, implemented from scratch using numpy. It doesn't make use of more optimization techniques for better performance like regularization, dropout or mini-batch gradient descent. That will be implemented in a newer version again\n",
    "\n",
    "the data is taking from [kaggle](https://www.kaggle.com/competitions/digit-recognizer/data), which contains all images flattened and present in a csv file. As convolution is not involved, this is ok for current purpose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17e29237-a4fa-4d5a-96cf-f0db600ccfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6b4cbd",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "\n",
    "the dataset directory contains the data downloaded directly as is from kaggle, without any modifications. So after loading it, we have to split it, scale the gray scale values b/w 0 to 1, and convert from pandas DF to numpy.NDarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1af55c28-147e-4416-aea1-b7ac396519c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 9, 1, ..., 2, 6, 0]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('./dataset/train.csv')\n",
    "train_data = np.array(train_data)\n",
    "test_data = pd.read_csv('./dataset/test.csv')\n",
    "test_data = np.array(test_data)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "np.random.shuffle(train_data)\n",
    "\n",
    "#separate data into training and cross validation set, and transpose so that data is column-wise, not row-wise\n",
    "cv = train_data[:3000].T\n",
    "train = train_data[3000:].T\n",
    "\n",
    "\n",
    "X_train = train[1:]\n",
    "X_train = X_train / 255 \n",
    "Y_train = train[0].reshape((1, -1))\n",
    "m = X_train.shape[1]\n",
    "\n",
    "X_cv = cv[1:]\n",
    "X_cv = X_cv / 255\n",
    "Y_cv = cv[0].reshape((1, -1))\n",
    "\n",
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1732a1-090f-4055-8bab-97039c0d4d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e027e12a-b2dd-46c3-af88-febf9538c7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x-np.max(x, axis=0, keepdims=True))\n",
    "    \n",
    "    return exp_x/np.sum(exp_x, axis=0, keepdims=True)\n",
    "    \n",
    "def sparse_categorical_cross_entropy_loss(predictions, y):\n",
    "    return -np.sum(y * np.log(predictions)) / y.size\n",
    "\n",
    "\n",
    "def get_prediction(A3):\n",
    "    return np.argmax(A3, axis=0, keepdims=True)\n",
    "\n",
    "def accuracy(prediction, Y):\n",
    "    return np.sum(prediction==Y)/Y.size\n",
    "\n",
    "def one_hot_encode(Y):\n",
    "    encoded_Y = np.zeros((10, Y.size))\n",
    "\n",
    "    encoded_Y[Y, np.arange(Y.size)] = 1\n",
    "    return encoded_Y\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be76c11-81fd-4de5-a0df-c65b9d470e83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "054d5b93-42ea-4789-bc92-ace84879ac47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights():\n",
    "    #random_sample returns a uniform distribution from 0 to 1, in atleast v2.2\n",
    "\n",
    "    W1 = np.random.random_sample((16, 784)) - 0.5\n",
    "    b1 = np.random.random_sample((16, 1)) - 0.5\n",
    "\n",
    "    W2 = np.random.random_sample((16, 16))- 0.5\n",
    "    b2 = np.random.random_sample((16, 1))- 0.5\n",
    "\n",
    "    W3 = np.random.random_sample((10, 16))- 0.5\n",
    "    b3 = np.random.random_sample((10, 1))- 0.5\n",
    "\n",
    "    return W1, b1, W2, b2, W3, b3\n",
    "\n",
    "def forward_prop(W1, b1, W2, b2, W3, b3, X):\n",
    "\n",
    "    Z1 = np.matmul(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "\n",
    "    Z2 = np.matmul(W2, A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "\n",
    "    Z3 = np.matmul(W3, A2) + b3\n",
    "    A3 = softmax(Z3)\n",
    "\n",
    "    return Z1, A1, Z2, A2, Z3, A3\n",
    "\n",
    "def back_prop(Z1, A1, Z2, A2, Z3, A3, W2, W3, X, y):\n",
    "    \"\"\"\n",
    "    Assuming y has shape (10, m)\n",
    "    \"\"\"\n",
    "    m = y.shape[1]\n",
    "\n",
    "    ## layer 3\n",
    "    # dA3 = -y / A3\n",
    "    dZ3 = A3 - y\n",
    "\n",
    "    dW3 = np.matmul(dZ3, A2.T) / m\n",
    "    db3 = np.sum(dZ3, axis=1, keepdims=True) / m\n",
    "\n",
    "    ## layer 2\n",
    "    dA2 = np.matmul(W3.T, dZ3) \n",
    "    dZ2 = dA2 * (Z2 > 0)\n",
    "\n",
    "    dW2 = np.matmul(dZ2, A1.T) / m\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n",
    "\n",
    "    ## layer 1\n",
    "    dA1 = np.matmul(W2.T, dZ2)\n",
    "    dZ1 = dA1 * (Z1 > 0)\n",
    "\n",
    "    dW1 = np.matmul(dZ1, X.T) / m\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
    "\n",
    "    return dW1, db1, dW2, db2, dW3, db3\n",
    "\n",
    "def update_params(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, alpha):\n",
    "\n",
    "    W1 -= alpha * dW1\n",
    "    b1 -= alpha * db1\n",
    "\n",
    "    W2 -= alpha * dW2\n",
    "    b2 -= alpha * db2\n",
    "\n",
    "    W3 -= alpha * dW3\n",
    "    b3 -= alpha * db3\n",
    "\n",
    "    return W1, b1, W2, b2, W3, b3\n",
    "\n",
    "def gradient_descent(X, Y, iterations, alpha, W1=None, b1=None, W2=None, b2=None, W3=None, b3=None):\n",
    "\n",
    "    new_y = one_hot_encode(Y)\n",
    "    if W1 is not None:\n",
    "        W1, b1, W2, b2, W3, b3 = initialize_weights()\n",
    "    \n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        Z1, A1, Z2, A2, Z3, A3 = forward_prop(W1, b1, W2, b2, W3, b3, X)\n",
    "        dW1, db1, dW2, db2, dW3, db3 = back_prop(Z1, A1, Z2, A2, Z3, A3, W2, W3, X, new_y)\n",
    "        W1, b1, W2, b2, W3, b3 = update_params(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, alpha)\n",
    "\n",
    "        if i%50==0:\n",
    "            \n",
    "            prediction = get_prediction(A3)\n",
    "            print(f\"Iteration: {i}, took {time.time()-start}s- Accuracy={accuracy(prediction, Y)}\")\n",
    "            start = time.time()\n",
    "            # print(\"Accuracy: \", accuracy(get_prediction(A3), Y))\n",
    "\n",
    "    return W1, b1, W2, b2, W3, b3\n",
    "\n",
    "        \n",
    "def make_prediction(W1, b1, W2, b2, W3, b3, X):\n",
    "    _, _, _, _, _, A3 = forward_prop(W1, b1, W2, b2, W3, b3, X)\n",
    "    predictions = get_prediction(A3)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8908f6e5-a836-4093-9be8-14301a23c357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, took 0.11981797218322754s- Accuracy=0.12007692307692308\n",
      "Iteration: 50, took 4.489748477935791s- Accuracy=0.5018717948717949\n",
      "Iteration: 100, took 4.544604063034058s- Accuracy=0.6758205128205128\n",
      "Iteration: 150, took 4.505732774734497s- Accuracy=0.7459743589743589\n",
      "Iteration: 200, took 4.580596685409546s- Accuracy=0.7837692307692308\n",
      "Iteration: 250, took 4.880459785461426s- Accuracy=0.8096923076923077\n",
      "Iteration: 300, took 4.668389797210693s- Accuracy=0.8284615384615385\n",
      "Iteration: 350, took 4.560608863830566s- Accuracy=0.8403333333333334\n",
      "Iteration: 400, took 4.733078479766846s- Accuracy=0.8465897435897436\n",
      "Iteration: 450, took 5.974888563156128s- Accuracy=0.8585128205128205\n",
      "Iteration: 500, took 2.0204713344573975s- Accuracy=0.8641538461538462\n",
      "Iteration: 550, took 1.673706293106079s- Accuracy=0.8694358974358974\n",
      "Iteration: 600, took 1.6700444221496582s- Accuracy=0.8743076923076923\n",
      "Iteration: 650, took 1.6652534008026123s- Accuracy=0.8788205128205128\n",
      "Iteration: 700, took 1.6363496780395508s- Accuracy=0.8825128205128205\n",
      "Iteration: 750, took 1.6339099407196045s- Accuracy=0.885948717948718\n",
      "Iteration: 800, took 1.6715061664581299s- Accuracy=0.8888461538461538\n",
      "Iteration: 850, took 1.6575939655303955s- Accuracy=0.8912051282051282\n",
      "Iteration: 900, took 1.7070260047912598s- Accuracy=0.8934358974358975\n",
      "Iteration: 950, took 1.715714693069458s- Accuracy=0.8956153846153846\n",
      "Iteration: 1000, took 1.8055152893066406s- Accuracy=0.8976666666666666\n",
      "Iteration: 1050, took 1.6582179069519043s- Accuracy=0.8998205128205128\n",
      "Iteration: 1100, took 1.7021491527557373s- Accuracy=0.9012564102564102\n",
      "Iteration: 1150, took 1.7137196063995361s- Accuracy=0.9026923076923077\n",
      "Iteration: 1200, took 1.8459913730621338s- Accuracy=0.903974358974359\n",
      "Iteration: 1250, took 1.8819611072540283s- Accuracy=0.9053846153846153\n",
      "Iteration: 1300, took 1.9281659126281738s- Accuracy=0.9072820512820513\n",
      "Iteration: 1350, took 1.8044257164001465s- Accuracy=0.9084871794871795\n",
      "Iteration: 1400, took 2.0251832008361816s- Accuracy=0.9097948717948718\n",
      "Iteration: 1450, took 2.0364861488342285s- Accuracy=0.911051282051282\n",
      "Iteration: 1500, took 1.9048893451690674s- Accuracy=0.9118461538461539\n",
      "Iteration: 1550, took 1.9573166370391846s- Accuracy=0.913025641025641\n",
      "Iteration: 1600, took 1.9983625411987305s- Accuracy=0.9136153846153846\n",
      "Iteration: 1650, took 1.9308326244354248s- Accuracy=0.9144871794871795\n",
      "Iteration: 1700, took 1.928328514099121s- Accuracy=0.9153076923076923\n",
      "Iteration: 1750, took 1.9059185981750488s- Accuracy=0.915974358974359\n",
      "Iteration: 1800, took 1.9617040157318115s- Accuracy=0.9167435897435897\n",
      "Iteration: 1850, took 1.9267957210540771s- Accuracy=0.9176923076923077\n",
      "Iteration: 1900, took 2.400825262069702s- Accuracy=0.9186153846153846\n",
      "Iteration: 1950, took 2.0209178924560547s- Accuracy=0.9192051282051282\n",
      "Iteration: 2000, took 2.045030355453491s- Accuracy=0.919923076923077\n",
      "Iteration: 2050, took 2.0345232486724854s- Accuracy=0.9201538461538461\n",
      "Iteration: 2100, took 2.026456117630005s- Accuracy=0.9208717948717948\n",
      "Iteration: 2150, took 2.3169472217559814s- Accuracy=0.9216410256410257\n",
      "Iteration: 2200, took 3.0390827655792236s- Accuracy=0.9223589743589744\n",
      "Iteration: 2250, took 2.6681628227233887s- Accuracy=0.9232051282051282\n",
      "Iteration: 2300, took 2.2695271968841553s- Accuracy=0.9238974358974359\n",
      "Iteration: 2350, took 3.7455694675445557s- Accuracy=0.9246153846153846\n",
      "Iteration: 2400, took 7.152831792831421s- Accuracy=0.9253333333333333\n",
      "Iteration: 2450, took 6.253759145736694s- Accuracy=0.9257692307692308\n",
      "Iteration: 2500, took 5.2229673862457275s- Accuracy=0.9262820512820513\n",
      "Iteration: 2550, took 6.026906490325928s- Accuracy=0.9267948717948717\n",
      "Iteration: 2600, took 6.5337231159210205s- Accuracy=0.9272051282051282\n",
      "Iteration: 2650, took 5.5991370677948s- Accuracy=0.9276923076923077\n",
      "Iteration: 2700, took 6.262112617492676s- Accuracy=0.9283846153846154\n",
      "Iteration: 2750, took 5.007707595825195s- Accuracy=0.928974358974359\n",
      "Iteration: 2800, took 4.522910118103027s- Accuracy=0.9295641025641026\n",
      "Iteration: 2850, took 4.399839878082275s- Accuracy=0.929974358974359\n",
      "Iteration: 2900, took 4.700065612792969s- Accuracy=0.9303076923076923\n",
      "Iteration: 2950, took 5.055557012557983s- Accuracy=0.9307435897435897\n",
      "Iteration: 3000, took 4.382749557495117s- Accuracy=0.9310512820512821\n",
      "Iteration: 3050, took 4.681489706039429s- Accuracy=0.9316666666666666\n",
      "Iteration: 3100, took 4.452158451080322s- Accuracy=0.931974358974359\n",
      "Iteration: 3150, took 4.604269027709961s- Accuracy=0.9322307692307692\n",
      "Iteration: 3200, took 4.551658391952515s- Accuracy=0.9325897435897436\n",
      "Iteration: 3250, took 4.500244140625s- Accuracy=0.933076923076923\n",
      "Iteration: 3300, took 4.367889642715454s- Accuracy=0.9334871794871795\n",
      "Iteration: 3350, took 4.563688516616821s- Accuracy=0.9339487179487179\n",
      "Iteration: 3400, took 4.518757343292236s- Accuracy=0.9343076923076923\n",
      "Iteration: 3450, took 4.421860218048096s- Accuracy=0.935025641025641\n",
      "Iteration: 3500, took 4.499571323394775s- Accuracy=0.9357179487179487\n",
      "Iteration: 3550, took 4.426841497421265s- Accuracy=0.935974358974359\n",
      "Iteration: 3600, took 4.384172201156616s- Accuracy=0.9363333333333334\n",
      "Iteration: 3650, took 4.476904630661011s- Accuracy=0.9365128205128205\n",
      "Iteration: 3700, took 4.427562236785889s- Accuracy=0.9368461538461539\n",
      "Iteration: 3750, took 4.370409965515137s- Accuracy=0.9374102564102564\n",
      "Iteration: 3800, took 4.6133623123168945s- Accuracy=0.937974358974359\n",
      "Iteration: 3850, took 4.444320201873779s- Accuracy=0.9383333333333334\n",
      "Iteration: 3900, took 4.4506800174713135s- Accuracy=0.9386923076923077\n",
      "Iteration: 3950, took 4.493626832962036s- Accuracy=0.9391794871794872\n",
      "Iteration: 4000, took 4.516199111938477s- Accuracy=0.9395128205128205\n",
      "Iteration: 4050, took 4.4658849239349365s- Accuracy=0.9396666666666667\n",
      "Iteration: 4100, took 4.448891639709473s- Accuracy=0.9403589743589743\n",
      "Iteration: 4150, took 4.362537384033203s- Accuracy=0.9406923076923077\n",
      "Iteration: 4200, took 4.353873014450073s- Accuracy=0.9409487179487179\n",
      "Iteration: 4250, took 5.001767635345459s- Accuracy=0.9414615384615385\n",
      "Iteration: 4300, took 5.192261219024658s- Accuracy=0.9416410256410256\n",
      "Iteration: 4350, took 5.229457855224609s- Accuracy=0.941974358974359\n",
      "Iteration: 4400, took 5.354613780975342s- Accuracy=0.9423589743589743\n",
      "Iteration: 4450, took 5.622851133346558s- Accuracy=0.9427692307692308\n",
      "Iteration: 4500, took 5.737837791442871s- Accuracy=0.9431794871794872\n",
      "Iteration: 4550, took 4.603846311569214s- Accuracy=0.9435641025641026\n",
      "Iteration: 4600, took 4.6388444900512695s- Accuracy=0.9437179487179487\n",
      "Iteration: 4650, took 4.659022331237793s- Accuracy=0.9438717948717948\n",
      "Iteration: 4700, took 4.686734914779663s- Accuracy=0.9441538461538461\n",
      "Iteration: 4750, took 5.184664964675903s- Accuracy=0.9445128205128205\n",
      "Iteration: 4800, took 5.059329986572266s- Accuracy=0.9446666666666667\n",
      "Iteration: 4850, took 4.562542676925659s- Accuracy=0.9448717948717948\n",
      "Iteration: 4900, took 4.50832724571228s- Accuracy=0.9452051282051283\n",
      "Iteration: 4950, took 4.680756330490112s- Accuracy=0.9455641025641026\n",
      "Iteration: 5000, took 5.114225149154663s- Accuracy=0.9457179487179487\n",
      "Iteration: 5050, took 5.519162654876709s- Accuracy=0.9462564102564103\n",
      "Iteration: 5100, took 4.54084324836731s- Accuracy=0.9465641025641026\n",
      "Iteration: 5150, took 4.4612507820129395s- Accuracy=0.9467435897435897\n",
      "Iteration: 5200, took 4.4244866371154785s- Accuracy=0.947\n",
      "Iteration: 5250, took 4.525253772735596s- Accuracy=0.9472051282051283\n",
      "Iteration: 5300, took 4.7697594165802s- Accuracy=0.9473846153846154\n",
      "Iteration: 5350, took 4.511694669723511s- Accuracy=0.9477692307692308\n",
      "Iteration: 5400, took 4.460360765457153s- Accuracy=0.948025641025641\n",
      "Iteration: 5450, took 4.442224502563477s- Accuracy=0.9482307692307692\n",
      "Iteration: 5500, took 4.561476230621338s- Accuracy=0.9484358974358974\n",
      "Iteration: 5550, took 4.660923957824707s- Accuracy=0.9486923076923077\n",
      "Iteration: 5600, took 4.434480905532837s- Accuracy=0.9487179487179487\n",
      "Iteration: 5650, took 4.396416425704956s- Accuracy=0.949025641025641\n",
      "Iteration: 5700, took 4.56389045715332s- Accuracy=0.9492307692307692\n",
      "Iteration: 5750, took 4.382213830947876s- Accuracy=0.9494615384615385\n",
      "Iteration: 5800, took 4.503571510314941s- Accuracy=0.9495897435897436\n",
      "Iteration: 5850, took 4.667494535446167s- Accuracy=0.9498461538461539\n",
      "Iteration: 5900, took 4.558439254760742s- Accuracy=0.9499487179487179\n",
      "Iteration: 5950, took 4.621626138687134s- Accuracy=0.9501025641025641\n",
      "Iteration: 6000, took 4.538080930709839s- Accuracy=0.9503076923076923\n",
      "Iteration: 6050, took 4.747119903564453s- Accuracy=0.9505128205128205\n",
      "Iteration: 6100, took 4.646218299865723s- Accuracy=0.9506923076923077\n",
      "Iteration: 6150, took 4.58823561668396s- Accuracy=0.9507435897435897\n",
      "Iteration: 6200, took 4.596682548522949s- Accuracy=0.9508974358974359\n",
      "Iteration: 6250, took 4.4726762771606445s- Accuracy=0.950974358974359\n",
      "Iteration: 6300, took 4.390740394592285s- Accuracy=0.9512051282051283\n",
      "Iteration: 6350, took 4.584246635437012s- Accuracy=0.9514615384615385\n",
      "Iteration: 6400, took 4.483783006668091s- Accuracy=0.9516153846153846\n",
      "Iteration: 6450, took 4.447234630584717s- Accuracy=0.9517435897435897\n",
      "Iteration: 6500, took 4.728216886520386s- Accuracy=0.9520512820512821\n",
      "Iteration: 6550, took 4.5981974601745605s- Accuracy=0.9522051282051283\n",
      "Iteration: 6600, took 4.527495622634888s- Accuracy=0.9522307692307692\n",
      "Iteration: 6650, took 4.383481502532959s- Accuracy=0.9524358974358974\n",
      "Iteration: 6700, took 4.400933027267456s- Accuracy=0.9526153846153846\n",
      "Iteration: 6750, took 4.360431432723999s- Accuracy=0.9527948717948718\n",
      "Iteration: 6800, took 4.6497602462768555s- Accuracy=0.9529230769230769\n",
      "Iteration: 6850, took 4.493082761764526s- Accuracy=0.9532307692307692\n",
      "Iteration: 6900, took 4.610446214675903s- Accuracy=0.9533333333333334\n",
      "Iteration: 6950, took 4.461359977722168s- Accuracy=0.9534615384615385\n",
      "Iteration: 7000, took 4.425796031951904s- Accuracy=0.9537948717948718\n",
      "Iteration: 7050, took 4.831537246704102s- Accuracy=0.954\n",
      "Iteration: 7100, took 4.4932074546813965s- Accuracy=0.9540512820512821\n",
      "Iteration: 7150, took 4.752370595932007s- Accuracy=0.9541282051282052\n",
      "Iteration: 7200, took 4.411028623580933s- Accuracy=0.9541538461538461\n",
      "Iteration: 7250, took 4.409528493881226s- Accuracy=0.9543589743589743\n",
      "Iteration: 7300, took 4.507287979125977s- Accuracy=0.9543846153846154\n",
      "Iteration: 7350, took 4.394605875015259s- Accuracy=0.9545384615384616\n",
      "Iteration: 7400, took 4.368846654891968s- Accuracy=0.9545897435897436\n",
      "Iteration: 7450, took 4.442366123199463s- Accuracy=0.9547692307692308\n",
      "Iteration: 7500, took 4.559899806976318s- Accuracy=0.9548205128205128\n",
      "Iteration: 7550, took 4.435471296310425s- Accuracy=0.9548205128205128\n",
      "Iteration: 7600, took 4.463433027267456s- Accuracy=0.9548717948717949\n",
      "Iteration: 7650, took 4.439659118652344s- Accuracy=0.9549230769230769\n",
      "Iteration: 7700, took 4.559441804885864s- Accuracy=0.9551282051282052\n",
      "Iteration: 7750, took 4.449754476547241s- Accuracy=0.9553846153846154\n",
      "Iteration: 7800, took 4.415979862213135s- Accuracy=0.9555384615384616\n",
      "Iteration: 7850, took 4.798727750778198s- Accuracy=0.9555641025641025\n",
      "Iteration: 7900, took 4.671628952026367s- Accuracy=0.9556666666666667\n",
      "Iteration: 7950, took 4.644675970077515s- Accuracy=0.9557435897435897\n",
      "Iteration: 8000, took 4.4665563106536865s- Accuracy=0.9558205128205128\n",
      "Iteration: 8050, took 4.532221794128418s- Accuracy=0.955974358974359\n",
      "Iteration: 8100, took 4.42666482925415s- Accuracy=0.9562051282051282\n",
      "Iteration: 8150, took 4.454045534133911s- Accuracy=0.9562820512820512\n",
      "Iteration: 8200, took 4.607568264007568s- Accuracy=0.9563076923076923\n",
      "Iteration: 8250, took 4.643460750579834s- Accuracy=0.9563076923076923\n",
      "Iteration: 8300, took 4.799596786499023s- Accuracy=0.9565384615384616\n",
      "Iteration: 8350, took 2.2537055015563965s- Accuracy=0.9566410256410256\n",
      "Iteration: 8400, took 1.9574742317199707s- Accuracy=0.9566923076923077\n",
      "Iteration: 8450, took 1.776597023010254s- Accuracy=0.9568974358974359\n",
      "Iteration: 8500, took 1.7971208095550537s- Accuracy=0.957025641025641\n",
      "Iteration: 8550, took 1.8192057609558105s- Accuracy=0.9572820512820512\n",
      "Iteration: 8600, took 1.8323323726654053s- Accuracy=0.9574615384615385\n",
      "Iteration: 8650, took 1.7942616939544678s- Accuracy=0.9574615384615385\n",
      "Iteration: 8700, took 1.7838413715362549s- Accuracy=0.9577948717948718\n",
      "Iteration: 8750, took 1.8056631088256836s- Accuracy=0.9578205128205128\n",
      "Iteration: 8800, took 1.8444573879241943s- Accuracy=0.9578205128205128\n",
      "Iteration: 8850, took 2.0273797512054443s- Accuracy=0.9578205128205128\n",
      "Iteration: 8900, took 2.1175413131713867s- Accuracy=0.9578461538461538\n",
      "Iteration: 8950, took 2.3788862228393555s- Accuracy=0.9581025641025641\n",
      "Iteration: 9000, took 2.0916101932525635s- Accuracy=0.9581025641025641\n",
      "Iteration: 9050, took 2.251481294631958s- Accuracy=0.9581282051282052\n",
      "Iteration: 9100, took 1.940107822418213s- Accuracy=0.9581794871794872\n",
      "Iteration: 9150, took 1.8013486862182617s- Accuracy=0.9583846153846154\n",
      "Iteration: 9200, took 1.9131629467010498s- Accuracy=0.9586666666666667\n",
      "Iteration: 9250, took 2.144814968109131s- Accuracy=0.9587435897435898\n",
      "Iteration: 9300, took 2.25891375541687s- Accuracy=0.9587179487179487\n",
      "Iteration: 9350, took 3.5889203548431396s- Accuracy=0.9588974358974359\n",
      "Iteration: 9400, took 4.87729549407959s- Accuracy=0.9588717948717949\n",
      "Iteration: 9450, took 4.752305507659912s- Accuracy=0.9590512820512821\n",
      "Iteration: 9500, took 5.1552088260650635s- Accuracy=0.9589487179487179\n",
      "Iteration: 9550, took 4.675970792770386s- Accuracy=0.9592051282051282\n",
      "Iteration: 9600, took 4.663388967514038s- Accuracy=0.9592051282051282\n",
      "Iteration: 9650, took 4.420442819595337s- Accuracy=0.9592051282051282\n",
      "Iteration: 9700, took 4.387454032897949s- Accuracy=0.9593589743589743\n",
      "Iteration: 9750, took 4.427492380142212s- Accuracy=0.9597435897435898\n",
      "Iteration: 9800, took 4.442672252655029s- Accuracy=0.9595897435897436\n",
      "Iteration: 9850, took 4.461325168609619s- Accuracy=0.9598205128205128\n",
      "Iteration: 9900, took 4.397784948348999s- Accuracy=0.959974358974359\n",
      "Iteration: 9950, took 4.393930435180664s- Accuracy=0.9601025641025641\n",
      "Iteration: 10000, took 4.448091268539429s- Accuracy=0.9601282051282052\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2, W3, b3 = gradient_descent(X_train, Y_train, 10001, 0.1, W1=W1, b1=b1, W2=W2, b2=b2, W3=W3, b3=b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05d3aecc-6545-4f3b-aa3b-d3216fb67756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.941)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_predictions = make_prediction(W1, b1, W2, b2, W3, b3, X_cv)\n",
    "\n",
    "accuracy(cv_predictions, Y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed9b65bf-c42c-4e72-9d85-99cc9b177935",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'W1':W1.tolist(),\n",
    "    'b1':b1.tolist(),\n",
    "    'W2':W2.tolist(),\n",
    "    'b2':b2.tolist(),\n",
    "    'W3':W3.tolist(),\n",
    "    'b3':b3.tolist()\n",
    "}\n",
    "import json\n",
    "with open('./weights_v1.json', 'w+') as f:\n",
    "    json.dump(weights, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "12b80cd5-6804-4547-922e-26634d42c887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label\n",
       "0      2\n",
       "1      0\n",
       "2      9\n",
       "3      4\n",
       "4      3"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_data = pd.read_csv('./dataset/test.csv')\n",
    "test_data = np.array(test_data)\n",
    "test_data = test_data.T / 255\n",
    "predictions = make_prediction(W1, b1, W2, b2, W3, b3, test_data)\n",
    "predictions = pd.DataFrame({'label':predictions.reshape((-1))})\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aef1662c-c339-433b-b017-a77f460fb591",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv(\"kaggle_submission_v1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5b8f30-5266-4f87-95ea-7b369f0376ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
