{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2523e1b",
   "metadata": {},
   "source": [
    "# 2.2 MNIST classifier\n",
    "This code implements a simple neural network of 2 hidden layers of 16 units each with ReLU activation, and 1 output layer of 10 units with softmax activation, implemented from scratch using numpy. It also makes use of regularization, mini-batch gradient descent and also momentum gradient descent and dropout regularization for more optimization. While version 2.1 didn't make use of dropout, this does\n",
    "\n",
    "the data is taking from [kaggle](https://www.kaggle.com/competitions/digit-recognizer/data), which contains all images flattened and present in a csv file. As convolution is not involved, this is ok for current purpose. \n",
    "\n",
    "\n",
    "### Results\n",
    "Applying dropout regularization only made the model worse. The first 2 results are without dropout, remaining with different tries of dropout, with different batch sizes. Increasing batch size can increase the CV accuracy, but also increase the epoch to get better result \n",
    "\n",
    "\n",
    "| Alpha | Beta | Lambda | Epoch |1-Dropout| Batch Size  |  Test Accuracy | Cross Validation Accuracy | \n",
    "|:-----:|:----:|:------:|:-----:|   :-:   |:-----------:|:--------------:|:-------------------------:|\n",
    "|    0.1| 0.9  | 0.0    |  500  |   1.0   |     64      |   99.2%        |          93.6%            |\n",
    "|    0.1| 0.9  | 0.01   |  100  |   1.0   |    128      |   97.2%        |          94.4%            |\n",
    "|    0.1| 0.9  | 0.0    |  500  |   0.8   |      64     |    96.3%       |          92.4%            |\n",
    "|    0.1| 0.9  | 0.0    |  500  |   0.5   |      64     |   91.5%        |          89.16%           |\n",
    "|    0.1| 0.9  | 0.0    |  500  |    0.8  |     128     |   96.77%       |          92.9%            |\n",
    "|   0.05| 0.9  | 0.0    |  500  |   0.7   |     128     |   93.9%        |          92.76%           |\n",
    "|   0.05| 0.9  | 0.0    |  1000 |   0.7   |     128     |   94.7%        |          92.26%           |\n",
    "|    0.1| 0.9  | 0.0    |  500  |   0.8   |     256     |   96.02%       |          93.16%           |\n",
    "|    0.1| 0.9  | 0.0    |  500  |   0.8   |     512     |   94.5%        |          92.3%            |\n",
    "|    0.1| 0.9  | 0.0    |  1000 |   0.8   |     512     |   95.2%        |          93.6%            |\n",
    "\n",
    "   \n",
    "\n",
    "-------------------\n",
    "### Github Instruction\n",
    "after downloading the files in parent directory, `/kaggle/mnist_classifier/version2`, extract `dataset.zip` into folder `dataset`. If you want to load in pretrained weights, use `weights_v2.1.json`. it contains a dict, with keys (W1, b1, ...) mapped with their matrices. Load them in and convert to np.array and you should be good to go (Although I haven't tested it, _yet_). It also contains a 'details' key with the hypterparameters during training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17e29237-a4fa-4d5a-96cf-f0db600ccfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6b4cbd",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "\n",
    "the dataset directory contains the data downloaded directly as is from kaggle, without any modifications. So after loading it, we have to split it, scale the gray scale values b/w 0 to 1, and convert from pandas DF to numpy.NDarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1af55c28-147e-4416-aea1-b7ac396519c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 9, 1, ..., 2, 6, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('./dataset/train.csv')\n",
    "train_data = np.array(train_data)\n",
    "test_data = pd.read_csv('./dataset/test.csv')\n",
    "test_data = np.array(test_data)\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "np.random.shuffle(train_data)\n",
    "\n",
    "#separate data into training and cross validation set, and transpose so that data is column-wise, not row-wise\n",
    "cv = train_data[:3000].T\n",
    "train = train_data[3000:].T\n",
    "\n",
    "\n",
    "X_train = train[1:]\n",
    "X_train = X_train / 255 \n",
    "Y_train = train[0].reshape((1, -1))\n",
    "m = X_train.shape[1]\n",
    "\n",
    "X_cv = cv[1:]\n",
    "X_cv = X_cv / 255\n",
    "Y_cv = cv[0].reshape((1, -1))\n",
    "\n",
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1732a1-090f-4055-8bab-97039c0d4d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e027e12a-b2dd-46c3-af88-febf9538c7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x-np.max(x, axis=0, keepdims=True))\n",
    "    \n",
    "    return exp_x/np.sum(exp_x, axis=0, keepdims=True)\n",
    "    \n",
    "def sparse_categorical_cross_entropy_loss(predictions, y):\n",
    "    return -np.sum(y * np.log(predictions)) / y.size\n",
    "\n",
    "\n",
    "def get_prediction(A3):\n",
    "    return np.argmax(A3, axis=0, keepdims=True)\n",
    "\n",
    "def accuracy(prediction, Y):\n",
    "    return np.sum(prediction==Y)/Y.size\n",
    "\n",
    "def one_hot_encode(Y):\n",
    "    encoded_Y = np.zeros((10, Y.size))\n",
    "\n",
    "    encoded_Y[Y, np.arange(Y.size)] = 1\n",
    "    return encoded_Y\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be76c11-81fd-4de5-a0df-c65b9d470e83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "054d5b93-42ea-4789-bc92-ace84879ac47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights():\n",
    "    #random_sample returns a uniform distribution from 0 to 1, in atleast v2.2\n",
    "\n",
    "    W1 = np.random.random_sample((16, 784)) - 0.5\n",
    "    b1 = np.random.random_sample((16, 1)) - 0.5\n",
    "\n",
    "    W2 = np.random.random_sample((16, 16))- 0.5\n",
    "    b2 = np.random.random_sample((16, 1))- 0.5\n",
    "\n",
    "    W3 = np.random.random_sample((10, 16))- 0.5\n",
    "    b3 = np.random.random_sample((10, 1))- 0.5\n",
    "\n",
    "    return W1, b1, W2, b2, W3, b3\n",
    "\n",
    "def forward_prop(W1, b1, W2, b2, W3, b3, X, keep_prob=1.0):\n",
    "    \n",
    "\n",
    "    Z1 = np.matmul(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    d1 = np.random.random_sample(A1.shape) < keep_prob\n",
    "    A1 *= d1 / keep_prob\n",
    "\n",
    "    Z2 = np.matmul(W2, A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "    d2 = np.random.random_sample(A2.shape) < keep_prob\n",
    "    A2 *= d2 / keep_prob\n",
    "\n",
    "    Z3 = np.matmul(W3, A2) + b3\n",
    "    A3 = softmax(Z3)\n",
    "\n",
    "    return Z1, A1, d1, Z2, A2, d2, Z3, A3\n",
    "\n",
    "def back_prop(Z1, A1, d1, Z2, A2, d2, Z3, A3, W1, W2, W3, X, y, lambda_=0.0):\n",
    "    \"\"\"\n",
    "    Assuming y has shape (10, m)\n",
    "    \"\"\"\n",
    "    m = y.shape[1]\n",
    "\n",
    "    ## layer 3\n",
    "    # dA3 = -y / A3\n",
    "    dZ3 = A3 - y\n",
    "\n",
    "    dW3 = np.matmul(dZ3, A2.T) / m + lambda_ / m * W3\n",
    "    db3 = np.sum(dZ3, axis=1, keepdims=True) / m\n",
    "\n",
    "    ## layer 2\n",
    "    dA2 = np.matmul(W3.T, dZ3) * d2\n",
    "    dZ2 = dA2 * (Z2 > 0)\n",
    "\n",
    "    dW2 = np.matmul(dZ2, A1.T) / m + lambda_ / m * W2\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n",
    "\n",
    "    ## layer 1\n",
    "    dA1 = np.matmul(W2.T, dZ2) * d1\n",
    "    dZ1 = dA1 * (Z1 > 0)\n",
    "\n",
    "    dW1 = np.matmul(dZ1, X.T) / m + lambda_/m * W1\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
    "\n",
    "    return dW1, db1, dW2, db2, dW3, db3\n",
    "\n",
    "def update_params(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, alpha):\n",
    "\n",
    "    W1 -= alpha * dW1\n",
    "    b1 -= alpha * db1\n",
    "\n",
    "    W2 -= alpha * dW2\n",
    "    b2 -= alpha * db2\n",
    "\n",
    "    W3 -= alpha * dW3\n",
    "    b3 -= alpha * db3\n",
    "\n",
    "    return W1, b1, W2, b2, W3, b3\n",
    "\n",
    "def gradient_descent(X, Y, epoch, alpha, lambda_=0.0, batch_size=64, beta=0.9, keep_prob=1.0, W1=None, b1=None, W2=None, b2=None, W3=None, b3=None):\n",
    "\n",
    "    n, m = X.shape\n",
    "    y_copy = one_hot_encode(Y)\n",
    "    print(y_copy.shape)\n",
    "    x_copy = X.copy()\n",
    "    \n",
    "    if W1 is None:\n",
    "        W1, b1, W2, b2, W3, b3 = initialize_weights()\n",
    "\n",
    "    v_dW1 = np.zeros_like(W1)\n",
    "    v_db1 = np.zeros_like(b1)\n",
    "    v_dW2 = np.zeros_like(W2)\n",
    "    v_db2 = np.zeros_like(b2)\n",
    "    v_dW3 = np.zeros_like(W3)\n",
    "    v_db3 = np.zeros_like(b3)\n",
    "    \n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    for i in range(epoch):\n",
    "\n",
    "        #this tracks the number of correct predictions done by the algorithm\n",
    "        y_correct = 0\n",
    "\n",
    "        for j in range(0, m, batch_size):\n",
    "            x_batch = x_copy[:, j: j+batch_size]\n",
    "            y_batch = y_copy[:, j: j+batch_size]\n",
    "            \n",
    "            Z1, A1, d1, Z2, A2, d2, Z3, A3 = forward_prop(W1, b1, W2, b2, W3, b3, x_batch, keep_prob=keep_prob)            \n",
    "            dW1, db1, dW2, db2, dW3, db3 = back_prop(Z1, A1, d1, Z2, A2, d2, Z3, A3, W1, W2, W3, x_batch, y_batch, lambda_)    \n",
    "            \n",
    "\n",
    "            v_dW1 = beta* v_dW1 + (1-beta)*dW1\n",
    "            v_db1 = beta* v_db1 + (1-beta)*db1\n",
    "            v_dW2 = beta* v_dW2 + (1-beta)*dW2\n",
    "            v_db2 = beta* v_db2 + (1-beta)*db2\n",
    "            v_dW3 = beta* v_dW3 + (1-beta)*dW3\n",
    "            v_db3 = beta* v_db3 + (1-beta)*db3\n",
    "            \n",
    "            W1, b1, W2, b2, W3, b3 = update_params(W1, b1, W2, b2, W3, b3, v_dW1, v_db1, v_dW2, v_db2, v_dW3, v_db3, alpha)\n",
    "\n",
    "            \n",
    "            prediction = get_prediction(A3)\n",
    "            \n",
    "            y_correct += np.sum(prediction == np.argmax(y_batch, axis=0, keepdims=True)) \n",
    "\n",
    "        if i%30==0 or i==epoch-1:\n",
    "            print(f\"Iteration: {i+1}, took {time.time()-start}s- Accuracy={y_correct/m}\")\n",
    "            start = time.time()\n",
    "\n",
    "    return W1, b1, W2, b2, W3, b3\n",
    "\n",
    "        \n",
    "def make_prediction(W1, b1, W2, b2, W3, b3, X):\n",
    "    _, _, _, _, _, _, _, A3 = forward_prop(W1, b1, W2, b2, W3, b3, X)\n",
    "    predictions = get_prediction(A3)\n",
    "    return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8908f6e5-a836-4093-9be8-14301a23c357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 39000)\n",
      "Iteration: 1, took 0.18317389488220215s- Accuracy=0.23553846153846153\n",
      "Iteration: 31, took 6.621882438659668s- Accuracy=0.773974358974359\n",
      "Iteration: 61, took 6.655176639556885s- Accuracy=0.8156410256410257\n",
      "Iteration: 91, took 5.816028833389282s- Accuracy=0.8337692307692308\n",
      "Iteration: 121, took 6.150511980056763s- Accuracy=0.8460512820512821\n",
      "Iteration: 151, took 6.219083309173584s- Accuracy=0.8528974358974359\n",
      "Iteration: 181, took 6.030574321746826s- Accuracy=0.8592051282051282\n",
      "Iteration: 211, took 6.018198728561401s- Accuracy=0.863051282051282\n",
      "Iteration: 241, took 6.113745927810669s- Accuracy=0.8668205128205129\n",
      "Iteration: 271, took 6.247676372528076s- Accuracy=0.8705384615384615\n",
      "Iteration: 301, took 6.163020610809326s- Accuracy=0.8714615384615385\n",
      "Iteration: 331, took 6.2688422203063965s- Accuracy=0.8761025641025642\n",
      "Iteration: 361, took 6.829492807388306s- Accuracy=0.8745384615384615\n",
      "Iteration: 391, took 6.800660610198975s- Accuracy=0.8764358974358974\n",
      "Iteration: 421, took 6.5669403076171875s- Accuracy=0.8773589743589744\n",
      "Iteration: 451, took 6.2445409297943115s- Accuracy=0.8778205128205128\n",
      "Iteration: 481, took 6.007771015167236s- Accuracy=0.8787692307692307\n",
      "Iteration: 511, took 6.10788106918335s- Accuracy=0.8811025641025642\n",
      "Iteration: 541, took 5.87947154045105s- Accuracy=0.8811025641025642\n",
      "Iteration: 571, took 6.378588914871216s- Accuracy=0.8836410256410256\n",
      "Iteration: 601, took 6.230408191680908s- Accuracy=0.8852051282051282\n",
      "Iteration: 631, took 6.067232131958008s- Accuracy=0.8834358974358975\n",
      "Iteration: 661, took 6.257421255111694s- Accuracy=0.8873846153846154\n",
      "Iteration: 691, took 6.195082426071167s- Accuracy=0.8858205128205128\n",
      "Iteration: 721, took 6.163103342056274s- Accuracy=0.8900769230769231\n",
      "Iteration: 750, took 6.028593301773071s- Accuracy=0.8877948717948718\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2, W3, b3 = gradient_descent(X_train, Y_train, epoch=750, alpha=0.1, keep_prob=0.8, lambda_=0.0, batch_size=512)# W1=W1,b1=b1,W2=W2,b2=b2,W3=W3,b3=b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "05d3aecc-6545-4f3b-aa3b-d3216fb67756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9523076923076923\n",
      "Test accuracy: 0.936\n"
     ]
    }
   ],
   "source": [
    "test_predictions = make_prediction(W1, b1, W2, b2, W3, b3, X_train)\n",
    "cv_predictions = make_prediction(W1, b1, W2, b2, W3, b3, X_cv)\n",
    "\n",
    "\n",
    "print(f\"Test accuracy: {accuracy(test_predictions, Y_train)}\")\n",
    "print(f\"Test accuracy: {accuracy(cv_predictions, Y_cv)}\")\n",
    "# accuracy(cv_predictions, Y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ed9b65bf-c42c-4e72-9d85-99cc9b177935",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'details':{\n",
    "        'learning_rate':0.1,\n",
    "        'lambda': 0.1,\n",
    "        'batch_size': 64,\n",
    "        'epoch':500,\n",
    "        'beta':0.9\n",
    "    },\n",
    "    'W1':W1.tolist(),\n",
    "    'b1':b1.tolist(),\n",
    "    'W2':W2.tolist(),\n",
    "    'b2':b2.tolist(),\n",
    "    'W3':W3.tolist(),\n",
    "    'b3':b3.tolist()\n",
    "}\n",
    "import json\n",
    "with open('./weights_v2.2.json', 'w+') as f:\n",
    "    json.dump(weights, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "12b80cd5-6804-4547-922e-26634d42c887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label\n",
       "0      2\n",
       "1      0\n",
       "2      9\n",
       "3      9\n",
       "4      3"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_data = pd.read_csv('./dataset/test.csv')\n",
    "test_data = np.array(test_data)\n",
    "test_data = test_data.T / 255\n",
    "predictions = make_prediction(W1, b1, W2, b2, W3, b3, test_data)\n",
    "predictions = pd.DataFrame({'label':predictions.reshape((-1))})\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aef1662c-c339-433b-b017-a77f460fb591",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv(\"kaggle_submission_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5b8f30-5266-4f87-95ea-7b369f0376ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
